{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1893,  0.3153,  0.1193,  ..., -0.2695, -0.4009,  0.4076],\n",
       "         [-1.1181, -0.1808,  0.6644,  ..., -0.8003, -0.3096,  0.4535],\n",
       "         [-0.6184, -0.2347,  0.3752,  ..., -0.6799, -0.4697,  0.6044],\n",
       "         ...,\n",
       "         [-0.2108,  0.1930,  0.2923,  ..., -0.7707, -0.5341,  0.5499],\n",
       "         [-0.4230,  0.4215,  0.0473,  ..., -1.1820, -0.2834,  0.7459],\n",
       "         [-0.1888,  0.3162,  0.1199,  ..., -0.2691, -0.4015,  0.4081]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 4.4811e-01, -4.0146e-01, -5.5674e-01,  1.3028e-01,  4.1158e-01,\n",
       "          1.9310e-02,  5.0401e-01, -2.9783e-01,  6.1256e-02, -2.0135e-01,\n",
       "          4.0959e-01,  4.5086e-02, -3.3453e-01,  1.4622e-01,  3.5792e-02,\n",
       "          5.9658e-01,  4.3783e-01, -4.9116e-01,  9.1487e-02,  3.7263e-01,\n",
       "         -2.8445e-01,  5.5821e-01,  2.5112e-01,  4.0286e-02, -1.2630e-01,\n",
       "          3.0461e-01,  1.9934e-01,  1.1804e-01,  5.9635e-01,  3.7724e-02,\n",
       "          5.6074e-02,  1.4698e-01,  2.0485e-01, -8.4490e-02, -2.8569e-01,\n",
       "         -1.0790e-01, -4.9942e-01,  1.2901e-01,  7.0174e-01, -2.2478e-01,\n",
       "         -4.3143e-01,  1.0639e-01, -3.3408e-02, -3.6333e-01,  1.5422e-01,\n",
       "          6.1477e-01,  1.8150e-01, -2.5103e-03, -2.3163e-01, -2.0268e-01,\n",
       "         -5.3928e-01,  4.7352e-01,  3.0646e-01,  1.5451e-01, -3.2498e-01,\n",
       "          1.2187e-01,  3.5065e-02, -1.7048e-01, -5.0242e-02, -2.9930e-01,\n",
       "         -4.4533e-01, -4.3013e-01,  1.0904e-01,  1.6585e-01, -9.1962e-02,\n",
       "         -2.1427e-01,  4.7716e-01, -6.0299e-04, -3.6422e-01,  1.4775e-01,\n",
       "         -3.5136e-01,  2.4957e-01,  4.6690e-02, -7.5652e-01, -7.5291e-02,\n",
       "          3.6267e-02, -5.2631e-01,  1.4291e-01,  6.2621e-01,  3.9675e-01,\n",
       "         -2.5591e-02,  2.9459e-01, -1.9668e-02,  1.4636e-01, -1.7168e-01,\n",
       "         -2.2709e-01,  6.0495e-01, -1.1798e-01,  1.6360e-01,  4.2524e-01,\n",
       "         -2.9935e-01, -6.9986e-01, -1.6568e-01,  2.8948e-01, -2.5667e-01,\n",
       "          2.5151e-01, -2.9652e-02,  1.7738e-01, -3.5677e-01, -6.9669e-02,\n",
       "          1.7095e-01, -1.7807e-01, -1.9805e-01,  2.6836e-01,  4.3840e-01,\n",
       "         -2.9026e-01, -2.1812e-01,  1.9209e-02,  1.4996e-01, -1.8026e-01,\n",
       "         -8.3071e-02,  6.3223e-01,  5.6915e-01,  1.9550e-01,  1.1567e-01,\n",
       "          1.5051e-01, -2.9030e-01, -3.9948e-01,  3.3763e-01, -3.7591e-01,\n",
       "          2.7872e-01, -1.6530e-01,  5.7950e-02,  2.4565e-01, -2.7960e-01,\n",
       "          2.2217e-01,  3.8426e-02,  4.7548e-01,  2.1708e-01, -1.0652e-01,\n",
       "          5.3701e-02, -3.1714e-02, -5.4813e-02,  1.9895e-01, -5.2264e-02,\n",
       "          4.6989e-02,  2.1754e-01, -7.7435e-01, -4.0996e-01,  5.7731e-01,\n",
       "          7.8618e-01,  1.4570e-01,  2.6977e-01,  1.8909e-01,  5.5266e-01,\n",
       "          6.0743e-01,  2.3274e-01, -5.3685e-01,  5.8790e-02,  3.0732e-01,\n",
       "         -9.5898e-02, -1.5794e-01, -3.0422e-01, -5.0524e-01, -6.1542e-01,\n",
       "         -1.2521e-04,  3.5888e-01, -1.2625e-02, -3.3812e-02,  6.1626e-01,\n",
       "          2.3905e-01, -4.6625e-01, -2.4046e-01, -1.4759e-01, -2.1509e-01,\n",
       "         -3.5956e-01, -1.4281e-01, -7.3188e-02, -4.6552e-01, -3.7559e-01,\n",
       "          5.4092e-03, -4.9872e-01, -2.6899e-02,  3.2335e-01, -5.5175e-01,\n",
       "          6.0252e-01, -4.9375e-01,  1.0769e-01,  5.3494e-01, -4.2550e-01,\n",
       "          1.1639e-01, -5.1151e-01, -6.1123e-02,  4.0013e-01,  4.3866e-03,\n",
       "          2.2162e-01, -2.2535e-01,  4.6588e-01, -3.9344e-02,  1.6756e-01,\n",
       "          2.6377e-01,  4.5937e-02, -3.0994e-01,  4.2681e-01, -4.7802e-01,\n",
       "          1.2067e-01, -3.7481e-01, -2.2174e-01, -3.3017e-01, -3.4768e-01,\n",
       "          2.4807e-01, -7.9806e-01, -5.9456e-01,  1.1379e-02, -2.0465e-01,\n",
       "          4.8156e-02, -7.2507e-02,  9.5063e-02,  8.1303e-02, -1.5863e-01,\n",
       "          1.3292e-01, -3.9718e-01,  4.4877e-01, -2.5761e-01, -1.8219e-01,\n",
       "         -6.6751e-02,  3.4143e-01,  4.0992e-01,  3.4393e-01, -4.9270e-01,\n",
       "         -4.1904e-01,  1.6818e-01,  4.7176e-01, -1.0362e-01,  5.9096e-01,\n",
       "         -1.1590e-01, -2.2539e-01,  7.1956e-02,  2.5673e-01,  1.8698e-01,\n",
       "          5.4770e-01, -3.1323e-01, -1.1896e-01,  5.3764e-02, -5.5752e-01,\n",
       "         -3.1938e-01, -2.4001e-01,  2.2472e-01,  5.0957e-01,  9.1827e-02,\n",
       "          2.0530e-01,  4.3938e-01,  3.7187e-01, -1.3636e-01,  4.4179e-01,\n",
       "         -1.9977e-01,  1.8337e-01, -5.6784e-01, -2.9557e-02, -5.5346e-01,\n",
       "         -3.1545e-01, -4.3282e-01,  6.8424e-01, -3.2818e-01,  5.0432e-01,\n",
       "          4.7508e-01, -3.1620e-01, -2.1925e-01,  2.1611e-01,  8.1507e-02,\n",
       "          2.1211e-01,  2.0374e-02,  9.9859e-02,  1.0547e-01,  8.7467e-02,\n",
       "          2.9928e-01,  5.9909e-01,  2.4319e-01,  3.8123e-01, -1.6471e-02,\n",
       "          1.2301e-02,  4.5534e-01, -1.7079e-01, -1.3880e-01, -6.5043e-02,\n",
       "          1.5669e-01, -2.3169e-01, -4.3629e-01, -2.0626e-02,  6.0027e-02,\n",
       "          6.5260e-01, -9.0950e-02, -3.3325e-01,  2.9754e-01,  2.4729e-01,\n",
       "         -5.6178e-01,  1.4152e-01,  1.8354e-01,  1.7322e-01, -4.7322e-01,\n",
       "         -5.6396e-02, -6.4685e-02,  2.5633e-01, -5.4523e-01, -4.9428e-01,\n",
       "          5.1916e-01, -4.5885e-02,  2.4006e-01,  2.4965e-01, -3.9579e-01,\n",
       "         -2.9866e-01,  7.2705e-01, -1.5416e-01, -5.3913e-01,  3.2787e-01,\n",
       "          2.1905e-01,  9.5815e-02,  1.9498e-01,  2.4061e-01,  3.6756e-01,\n",
       "         -2.3073e-01,  5.1622e-01,  2.5404e-01, -5.4435e-01, -4.5983e-01,\n",
       "         -1.1807e-01,  2.4593e-02, -2.0090e-02, -3.4364e-01, -5.1286e-01,\n",
       "          2.7917e-02,  1.7620e-01, -1.2510e-01,  3.0384e-01,  2.4185e-01,\n",
       "          3.4801e-01,  4.3028e-02,  4.5162e-01, -3.1206e-01,  5.6231e-01,\n",
       "         -3.0840e-02,  5.8086e-01, -5.0635e-01, -1.4054e-01, -2.2023e-02,\n",
       "         -1.5958e-01,  3.3143e-02, -5.8908e-02,  4.7322e-01, -2.8300e-01,\n",
       "         -4.7629e-01,  1.4241e-01,  2.7694e-01,  2.6623e-01,  3.1673e-01,\n",
       "          4.0701e-01,  2.6128e-01,  1.8798e-01,  1.4841e-01,  3.1568e-01,\n",
       "          2.7182e-01, -2.7031e-01, -6.7842e-01,  3.9033e-01, -4.1374e-01,\n",
       "         -5.2908e-02, -2.6605e-01, -4.0900e-01,  6.6715e-01, -4.1434e-01,\n",
       "          2.5158e-01,  4.5197e-01,  2.5152e-01,  2.1109e-01,  9.6926e-02,\n",
       "          2.8140e-01,  1.9675e-01,  8.6543e-02,  1.6715e-01,  2.4015e-02,\n",
       "         -3.8052e-01, -3.0755e-01, -1.4113e-01,  1.8012e-01, -3.5317e-01,\n",
       "         -4.9954e-01,  1.3343e-01,  5.7155e-01,  1.1180e-01, -2.9281e-01,\n",
       "          4.0434e-01,  3.6477e-01,  1.9866e-01, -3.2489e-02, -2.5354e-01,\n",
       "         -1.5786e-02,  6.5423e-01, -2.9574e-02,  1.6417e-01,  7.3892e-01,\n",
       "          2.9293e-01, -4.6866e-01, -9.7446e-02, -2.1532e-01,  8.2163e-02,\n",
       "          7.5897e-02, -3.9645e-01,  2.4703e-01,  5.0824e-01,  2.8530e-01,\n",
       "          7.4705e-01,  9.8639e-02,  8.4650e-02,  1.5404e-01,  2.2329e-01,\n",
       "         -2.0876e-02,  1.8073e-01,  3.3611e-02,  5.3089e-01,  4.8651e-01,\n",
       "         -4.3987e-01,  1.3626e-01, -2.0190e-01, -1.1382e-01, -2.6240e-01,\n",
       "         -3.9201e-01, -9.5006e-02,  4.2187e-01, -5.4713e-01, -1.9323e-02,\n",
       "         -1.1897e-01,  1.4893e-01, -8.6273e-03, -1.4618e-01, -2.1659e-01,\n",
       "         -3.4322e-01,  6.5785e-01, -7.6009e-02, -2.7382e-02, -4.9159e-01,\n",
       "         -3.5823e-01,  8.6719e-02,  3.6552e-01, -3.7448e-01, -2.2372e-01,\n",
       "          4.8316e-01,  7.4806e-03, -1.6359e-01, -2.3772e-01,  3.3843e-01,\n",
       "         -1.6232e-01,  1.8144e-01,  3.1105e-01, -1.5933e-01, -2.3729e-01,\n",
       "          5.4749e-02, -3.7692e-01, -1.8616e-01, -5.4574e-01,  5.2105e-01,\n",
       "         -4.1485e-01, -2.0894e-01, -2.4048e-01, -6.3111e-01,  5.4079e-02,\n",
       "          2.5172e-01,  4.7148e-01, -2.3521e-01,  1.3249e-01,  8.0833e-01,\n",
       "         -5.9235e-02, -4.1565e-01, -3.1158e-02,  4.1164e-01, -1.2552e-03,\n",
       "          6.3465e-01,  4.9709e-01,  3.8112e-04,  1.4461e-01,  6.7827e-01,\n",
       "          2.4835e-02,  2.5499e-01, -2.4876e-01,  5.3896e-01, -6.8879e-02,\n",
       "          2.1386e-01,  6.5747e-02,  8.3814e-02,  6.4292e-02, -2.1096e-01,\n",
       "          3.7549e-01,  5.0591e-01, -6.8542e-01, -1.5382e-01,  2.2022e-01,\n",
       "         -2.9040e-02, -2.0233e-01, -4.1894e-01, -2.7235e-01, -2.9858e-01,\n",
       "         -1.6754e-01, -1.7863e-01, -5.3560e-02, -4.9947e-01, -1.3371e-01,\n",
       "          2.4022e-01, -4.3763e-01,  1.5124e-01,  5.5221e-01,  1.2865e-02,\n",
       "          3.2393e-01, -3.4656e-01, -9.7426e-02,  3.2859e-01, -1.4154e-01,\n",
       "         -4.2817e-01,  1.1420e-01,  8.4534e-01, -2.8396e-01, -7.5250e-01,\n",
       "         -1.0830e-01,  4.2208e-01,  1.6780e-01,  1.2693e-01, -1.6561e-01,\n",
       "         -3.2127e-01,  1.1726e-01, -7.3592e-03,  1.4245e-01, -2.4554e-01,\n",
       "         -5.7179e-01, -2.5818e-01,  5.6322e-01, -6.4418e-01,  4.4862e-02,\n",
       "         -2.2628e-01,  4.2417e-02, -4.4835e-01,  2.8228e-01, -3.1715e-01,\n",
       "          6.9162e-01,  2.0521e-01, -5.7029e-01,  1.3523e-01,  1.1917e-01,\n",
       "         -4.0261e-02, -1.7166e-01, -2.6099e-01,  7.3334e-01, -4.2819e-01,\n",
       "         -7.9040e-01,  1.4441e-01,  2.9120e-01,  6.0029e-01, -3.1493e-01,\n",
       "         -9.3089e-02, -1.6218e-01,  2.6926e-01,  2.0315e-01, -1.3151e-02,\n",
       "         -2.9449e-01, -2.1290e-01, -5.2155e-01, -2.6552e-01, -4.6942e-01,\n",
       "          1.0255e-01,  7.8307e-02, -6.1092e-01,  2.2313e-01, -1.9430e-01,\n",
       "          2.3846e-01,  2.5205e-01, -3.8003e-01, -5.2030e-02,  3.5877e-01,\n",
       "          4.7537e-01,  2.3846e-01,  5.2688e-01,  1.8103e-01,  2.3641e-01,\n",
       "         -2.0798e-01,  1.9678e-01,  1.4144e-01, -7.4162e-02,  4.1308e-01,\n",
       "          2.9839e-03, -6.5521e-01, -8.3264e-02,  7.5988e-01,  1.1867e-01,\n",
       "         -3.4725e-01, -3.7191e-03,  5.4613e-01,  1.5159e-01,  8.4783e-02,\n",
       "          2.3284e-01, -3.9057e-01, -2.3409e-01, -7.3637e-02, -8.8615e-02,\n",
       "         -3.7052e-01, -2.9122e-01, -7.3432e-02, -1.9690e-01, -3.3851e-01,\n",
       "         -8.8222e-02, -2.3375e-01,  4.3049e-01, -5.8857e-01, -2.0399e-01,\n",
       "         -1.2193e-01, -9.2677e-02, -3.0028e-01,  1.7255e-01,  2.2437e-01,\n",
       "         -4.5415e-02,  6.0592e-02,  6.2751e-01, -1.0134e-01, -2.0221e-01,\n",
       "         -1.7402e-01, -2.1254e-01,  3.0357e-01, -2.1485e-01,  1.5480e-01,\n",
       "         -1.3759e-01,  3.2655e-01, -6.4297e-01, -2.2493e-01, -8.2992e-02,\n",
       "         -2.5047e-01, -4.9766e-01,  4.2412e-01,  2.3970e-01,  1.4987e-01,\n",
       "          5.4167e-01,  1.1718e-01,  6.6693e-02, -3.8581e-01, -4.2947e-01,\n",
       "         -1.2672e-01,  2.4945e-01,  5.9705e-03, -4.4449e-01, -2.3423e-01,\n",
       "          2.8919e-01, -4.3473e-01, -2.3519e-01,  3.4721e-01,  1.8874e-02,\n",
       "         -1.7171e-01, -2.8121e-01, -1.4597e-01, -6.7745e-01,  5.4171e-02,\n",
       "          2.1510e-01,  5.0583e-02, -2.1296e-01,  8.6429e-02, -1.5324e-01,\n",
       "          1.1508e-01,  2.3517e-01, -6.0331e-03, -8.5155e-02, -4.4437e-01,\n",
       "         -5.0531e-01, -2.1230e-01,  1.4810e-01,  3.6474e-01, -1.8075e-01,\n",
       "         -2.5538e-01,  2.0796e-01,  3.5243e-01, -1.6703e-01,  4.5286e-02,\n",
       "          7.9011e-02, -6.9788e-01, -2.0796e-01, -4.1728e-02,  2.0548e-01,\n",
       "         -1.1670e-02, -2.7418e-01, -4.8275e-01,  2.9674e-01, -8.9291e-02,\n",
       "         -2.2635e-01,  5.4526e-01, -3.4139e-01,  3.4212e-01,  3.6058e-02,\n",
       "         -3.7963e-01, -1.3085e-01,  4.2631e-01, -8.2411e-03,  3.8917e-01,\n",
       "          6.0390e-02, -5.9342e-01, -1.3966e-01, -1.2896e-02, -2.9506e-01,\n",
       "         -3.4779e-01, -1.6541e-01, -1.6342e-01,  3.6396e-01, -5.8852e-01,\n",
       "          4.1499e-01,  1.5367e-01,  1.6936e-01,  2.5519e-01, -4.0516e-01,\n",
       "         -2.9844e-01,  3.1877e-01,  4.0460e-01, -2.2433e-01, -4.7703e-01,\n",
       "         -4.6436e-01, -3.1306e-01, -4.1309e-01, -2.7383e-01,  5.6529e-01,\n",
       "         -1.4357e-01, -2.9513e-01, -2.4179e-01,  5.0537e-01,  1.7372e-01,\n",
       "          1.0630e-01,  3.6101e-01,  2.3137e-01,  1.6398e-01,  2.2440e-01,\n",
       "         -7.0337e-01,  2.4358e-01, -3.8856e-01,  3.1946e-02, -1.0649e-01,\n",
       "          2.1183e-01, -2.5591e-01,  1.0570e-01, -3.0648e-01,  1.5120e-01,\n",
       "          4.3427e-01, -3.8423e-01, -6.6168e-02,  3.1437e-01,  2.4464e-01,\n",
       "         -2.7369e-01,  1.4974e-01,  2.3342e-01,  4.5072e-01,  1.5830e-01,\n",
       "          2.1100e-01,  4.5603e-01, -4.2899e-01, -2.0731e-02, -4.4673e-01,\n",
       "         -5.1063e-01,  1.5111e-01,  1.8950e-01,  4.2561e-01, -1.3958e-01,\n",
       "         -3.2266e-01,  6.5264e-01,  3.3180e-02, -8.0326e-02,  3.3690e-01,\n",
       "          3.7761e-02, -2.5998e-02, -2.3861e-01,  3.5275e-01,  5.8248e-01,\n",
       "         -8.2065e-02, -1.0100e-01,  4.5963e-01, -5.5888e-01,  3.9027e-01,\n",
       "         -3.1220e-02, -5.3996e-03,  1.2802e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tokenizer(\"def print_hello_world(): print('Hello World')\", return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  9232,  5780,  1215, 42891,  1215,  8331, 49536,  5780, 45803,\n",
       "         31414,   623, 27645,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"def print_hello_world(): print('Hello World')\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.gitignore',\n",
       " '.idea',\n",
       " 'App.config',\n",
       " 'bin',\n",
       " 'Form1.cs',\n",
       " 'Form1.Designer.cs',\n",
       " 'Form1.resx',\n",
       " 'LifeSimulation.csproj',\n",
       " 'LifeSimulation.csproj.DotSettings',\n",
       " 'LifeSimulation.sln',\n",
       " 'myCs',\n",
       " 'obj',\n",
       " 'Program.cs',\n",
       " 'Properties',\n",
       " 'README.md',\n",
       " 'sources']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"/Users/79138/Mynka/hw/2. second grade/oop/myCs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12dfad86f805d2d41962782fe4cb74c1c6bdc3dd8d611f50435d9035ef53657e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
