{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>method</th>\n",
       "      <th>code_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def create_dataset_one_type(code_type):\\n    i...</td>\n",
       "      <td>PYTHON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def create_dataset():\\n    dfs = []\\n    for c...</td>\n",
       "      <td>PYTHON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_class_body(lines):\\n    in_class = Fal...</td>\n",
       "      <td>PYTHON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_methods(lines):\\n    functions = []\\n ...</td>\n",
       "      <td>PYTHON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_all_data():\\n    files = get_files('.c...</td>\n",
       "      <td>PYTHON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_methods(lines):\\n    functions = []\\n ...</td>\n",
       "      <td>PYTHON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_all_data():\\n    files = get_files('.p...</td>\n",
       "      <td>PYTHON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def print_hello_world():\\n    print(\"Hello wor...</td>\n",
       "      <td>PYTHON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def print_hello_world_many_times(n):\\n    for ...</td>\n",
       "      <td>PYTHON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\ut...</td>\n",
       "      <td>def get_files(format):\\n    files = []\\n    fo...</td>\n",
       "      <td>PYTHON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "1  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "2  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "3  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "4  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "5  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "6  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "7  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "8  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "9  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\ut...   \n",
       "\n",
       "                                              method code_type  \n",
       "0  def create_dataset_one_type(code_type):\\n    i...    PYTHON  \n",
       "1  def create_dataset():\\n    dfs = []\\n    for c...    PYTHON  \n",
       "2  def get_class_body(lines):\\n    in_class = Fal...    PYTHON  \n",
       "3  def get_methods(lines):\\n    functions = []\\n ...    PYTHON  \n",
       "4  def get_all_data():\\n    files = get_files('.c...    PYTHON  \n",
       "5  def get_methods(lines):\\n    functions = []\\n ...    PYTHON  \n",
       "6  def get_all_data():\\n    files = get_files('.p...    PYTHON  \n",
       "7  def print_hello_world():\\n    print(\"Hello wor...    PYTHON  \n",
       "8  def print_hello_world_many_times(n):\\n    for ...    PYTHON  \n",
       "9  def get_files(format):\\n    files = []\\n    fo...    PYTHON  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('result.csv', index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_hello_world():\n",
      "    print(\"Hello world!\")\n"
     ]
    }
   ],
   "source": [
    "print(data.loc[7, 'method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b00451d9ad445658c975f088ef44755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>method</th>\n",
       "      <th>code_type</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def create_dataset_one_type(code_type):\\n    i...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def create_dataset():\\n    dfs = []\\n    for c...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_class_body(lines):\\n    in_class = Fal...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_methods(lines):\\n    functions = []\\n ...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_all_data():\\n    files = get_files('.c...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_methods(lines):\\n    functions = []\\n ...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_all_data():\\n    files = get_files('.p...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def print_hello_world():\\n    print(\"Hello wor...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def print_hello_world_many_times(n):\\n    for ...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\ut...</td>\n",
       "      <td>def get_files(format):\\n    files = []\\n    fo...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "1  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "2  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "3  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "4  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "5  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "6  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "7  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "8  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "9  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\ut...   \n",
       "\n",
       "                                              method code_type  length  \n",
       "0  def create_dataset_one_type(code_type):\\n    i...    PYTHON      54  \n",
       "1  def create_dataset():\\n    dfs = []\\n    for c...    PYTHON     109  \n",
       "2  def get_class_body(lines):\\n    in_class = Fal...    PYTHON     169  \n",
       "3  def get_methods(lines):\\n    functions = []\\n ...    PYTHON     315  \n",
       "4  def get_all_data():\\n    files = get_files('.c...    PYTHON     223  \n",
       "5  def get_methods(lines):\\n    functions = []\\n ...    PYTHON     216  \n",
       "6  def get_all_data():\\n    files = get_files('.p...    PYTHON     197  \n",
       "7  def print_hello_world():\\n    print(\"Hello wor...    PYTHON      19  \n",
       "8  def print_hello_world_many_times(n):\\n    for ...    PYTHON      39  \n",
       "9  def get_files(format):\\n    files = []\\n    fo...    PYTHON     208  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['length'] = [len(tokenizer(elem)['input_ids']) for elem in tqdm(data['method'])]\n",
    "data = data[(data.length > 3) & (data.length < 512)].reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86de40e926e94fe5b1a34f679ddec996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    data['emb'] = [model(**tokenizer(elem, return_tensors='pt'))['pooler_output'].detach().cpu()[0] for elem in tqdm(data['method'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CosineSimilarity\n",
    "cos = CosineSimilarity(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"stop spending my time\"\n",
    "with torch.no_grad():\n",
    "    query = model(**tokenizer(query, return_tensors='pt'))['pooler_output'].detach()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccadac17291480a8b1b68b46049008d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sims = []\n",
    "for i in tqdm(data.emb):\n",
    "    sims.append(float(cos(query, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>method</th>\n",
       "      <th>code_type</th>\n",
       "      <th>length</th>\n",
       "      <th>emb</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def print_hello_world():\\n    print(\"Hello wor...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>19</td>\n",
       "      <td>[tensor(0.4202), tensor(-0.4082), tensor(-0.58...</td>\n",
       "      <td>0.990421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def print_hello_world_many_times(n):\\n    for ...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>39</td>\n",
       "      <td>[tensor(0.5079), tensor(-0.4202), tensor(-0.59...</td>\n",
       "      <td>0.981166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def create_dataset_one_type(code_type):\\n    i...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>54</td>\n",
       "      <td>[tensor(0.5141), tensor(-0.5052), tensor(-0.62...</td>\n",
       "      <td>0.971706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def create_dataset():\\n    dfs = []\\n    for c...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>109</td>\n",
       "      <td>[tensor(0.5184), tensor(-0.5192), tensor(-0.64...</td>\n",
       "      <td>0.948649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_class_body(lines):\\n    in_class = Fal...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>169</td>\n",
       "      <td>[tensor(0.5716), tensor(-0.5451), tensor(-0.66...</td>\n",
       "      <td>0.934011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_all_data():\\n    files = get_files('.p...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>197</td>\n",
       "      <td>[tensor(0.5544), tensor(-0.5402), tensor(-0.69...</td>\n",
       "      <td>0.923922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\ut...</td>\n",
       "      <td>def get_files(format):\\n    files = []\\n    fo...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>208</td>\n",
       "      <td>[tensor(0.5406), tensor(-0.5429), tensor(-0.66...</td>\n",
       "      <td>0.918905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_methods(lines):\\n    functions = []\\n ...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>216</td>\n",
       "      <td>[tensor(0.5321), tensor(-0.5386), tensor(-0.69...</td>\n",
       "      <td>0.915369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_all_data():\\n    files = get_files('.c...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>223</td>\n",
       "      <td>[tensor(0.5559), tensor(-0.5357), tensor(-0.68...</td>\n",
       "      <td>0.914659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...</td>\n",
       "      <td>def get_methods(lines):\\n    functions = []\\n ...</td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>315</td>\n",
       "      <td>[tensor(0.5400), tensor(-0.5345), tensor(-0.70...</td>\n",
       "      <td>0.906168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "1  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "2  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "3  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "4  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "5  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "6  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\ut...   \n",
       "7  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "8  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "9  c:\\Users\\79138\\Mynka\\no-hw\\voice helper\\src\\cr...   \n",
       "\n",
       "                                              method code_type  length  \\\n",
       "0  def print_hello_world():\\n    print(\"Hello wor...    PYTHON      19   \n",
       "1  def print_hello_world_many_times(n):\\n    for ...    PYTHON      39   \n",
       "2  def create_dataset_one_type(code_type):\\n    i...    PYTHON      54   \n",
       "3  def create_dataset():\\n    dfs = []\\n    for c...    PYTHON     109   \n",
       "4  def get_class_body(lines):\\n    in_class = Fal...    PYTHON     169   \n",
       "5  def get_all_data():\\n    files = get_files('.p...    PYTHON     197   \n",
       "6  def get_files(format):\\n    files = []\\n    fo...    PYTHON     208   \n",
       "7  def get_methods(lines):\\n    functions = []\\n ...    PYTHON     216   \n",
       "8  def get_all_data():\\n    files = get_files('.c...    PYTHON     223   \n",
       "9  def get_methods(lines):\\n    functions = []\\n ...    PYTHON     315   \n",
       "\n",
       "                                                 emb       sim  \n",
       "0  [tensor(0.4202), tensor(-0.4082), tensor(-0.58...  0.990421  \n",
       "1  [tensor(0.5079), tensor(-0.4202), tensor(-0.59...  0.981166  \n",
       "2  [tensor(0.5141), tensor(-0.5052), tensor(-0.62...  0.971706  \n",
       "3  [tensor(0.5184), tensor(-0.5192), tensor(-0.64...  0.948649  \n",
       "4  [tensor(0.5716), tensor(-0.5451), tensor(-0.66...  0.934011  \n",
       "5  [tensor(0.5544), tensor(-0.5402), tensor(-0.69...  0.923922  \n",
       "6  [tensor(0.5406), tensor(-0.5429), tensor(-0.66...  0.918905  \n",
       "7  [tensor(0.5321), tensor(-0.5386), tensor(-0.69...  0.915369  \n",
       "8  [tensor(0.5559), tensor(-0.5357), tensor(-0.68...  0.914659  \n",
       "9  [tensor(0.5400), tensor(-0.5345), tensor(-0.70...  0.906168  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = data.iloc[np.argsort(sims)[:-12:-1]].reset_index(drop=True)\n",
    "res['sim'] = np.array(sims)[np.argsort(sims)[:-12:-1]]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_all_data():\n",
      "    files = get_files('.py')\n",
      "    methods = []\n",
      "    filenames = []\n",
      "    for file in files:\n",
      "        with file.open('r') as f:\n",
      "            lines = f.read()\n",
      "        lines = lines.split('\\n')\n",
      "        lines = [line for line in lines if len(line) > 0]\n",
      "        plus = get_methods(lines)\n",
      "        methods += plus\n",
      "        filenames += [str(file)] * len(plus)\n",
      "    return pd.DataFrame({'filename': filenames, 'method': methods})\n"
     ]
    }
   ],
   "source": [
    "print(res.iloc[5]['method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CodeBERTForCodeGeneration' from 'transformers' (c:\\Users\\79138\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\79138\\Mynka\\no-hw\\voice helper\\experiments.ipynb Ячейка 12\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/79138/Mynka/no-hw/voice%20helper/experiments.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/79138/Mynka/no-hw/voice%20helper/experiments.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m CodeBERTForCodeGeneration, CodeBERTTokenizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/79138/Mynka/no-hw/voice%20helper/experiments.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Загрузка модели и токенайзера\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/79138/Mynka/no-hw/voice%20helper/experiments.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m CodeBERTForCodeGeneration\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mhuggingface/codebert-base-c\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'CodeBERTForCodeGeneration' from 'transformers' (c:\\Users\\79138\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CodeBERTForCodeGeneration, CodeBERTTokenizer\n",
    "\n",
    "# Загрузка модели и токенайзера\n",
    "model = CodeBERTForCodeGeneration.from_pretrained('huggingface/codebert-base-c')\n",
    "tokenizer = CodeBERTTokenizer.from_pretrained('huggingface/codebert-base-c')\n",
    "\n",
    "# Преобразование входных данных в тензоры\n",
    "prompt = \"print('Hello, world!')\"\n",
    "input_ids = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)  # Добавление батч-размерности\n",
    "\n",
    "# Генерация кода\n",
    "output = model.generate(input_ids)\n",
    "generated_code = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "любовь сытый страсть суши\n",
      "суши любовь роллы тунец\n",
      "страсть суши рис страсть\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Список слов, из которых будет формироваться стих\n",
    "words = ['суши', 'роллы', 'нори', 'тунец', 'рис', 'вкусное', 'страсть', 'любовь', 'сытый', 'счастливый']\n",
    "\n",
    "# Формируем стих, состоящий из трех строк\n",
    "verse = []\n",
    "for i in range(3):\n",
    "    # Формируем строку, состоящую из 4 случайных слов\n",
    "    line = []\n",
    "    for j in range(4):\n",
    "        line.append(random.choice(words))\n",
    "    # Объединяем слова в строку с помощью пробела и добавляем знак переноса строки\n",
    "    verse.append(' '.join(line) + '\\n')\n",
    "\n",
    "# Выводим стих на экран\n",
    "print(''.join(verse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\79138\\Mynka\\no-hw\\voice helper\\experiments.ipynb Ячейка 14\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/79138/Mynka/no-hw/voice%20helper/experiments.ipynb#X32sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Разделяем стих на строфы\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/79138/Mynka/no-hw/voice%20helper/experiments.ipynb#X32sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m num_stanzas \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m2\u001b[39m, \u001b[39m4\u001b[39m)  \u001b[39m# Количество строф будет случайным в диапазоне от 2 до 4\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/79138/Mynka/no-hw/voice%20helper/experiments.ipynb#X32sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m stanza_length \u001b[39m=\u001b[39m num_lines \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m num_\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Список слов, из которых будет формироваться стих\n",
    "words = ['суши', 'роллы', 'нори', 'тунец', 'рис', 'вкусное', 'страсть', 'любовь', 'сытый', 'счастливый']\n",
    "\n",
    "# Формируем стих, состоящий из случайного количества строк\n",
    "verse = []\n",
    "num_lines = random.randint(3, 6)  # Количество строк будет случайным в диапазоне от 3 до 6\n",
    "for i in range(num_lines):\n",
    "    # Формируем строку, состоящую из 4 случайных слов\n",
    "    line = []\n",
    "    for j in range(4):\n",
    "        line.append(random.choice(words))\n",
    "    # Объединяем слова в строку с помощью пробела и добавляем знак переноса строки\n",
    "    verse.append(' '.join(line) + '\\n')\n",
    "\n",
    "# Разделяем стих на строфы\n",
    "num_stanzas = random.randint(2, 4)  # Количество строф будет случайным в диапазоне от 2 до 4\n",
    "stanza_length = num_lines // num_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relish with, tasty the udon so vine,\n",
      "Flavorful nigiri, devour the udon so vine,\n",
      "Relish with, aromatic the maki so nine,\n",
      "Udon enjoy, eagerly beside maki so fine\n",
      "\n",
      "Смаковать с, пикантный the удон so девять,\n",
      "Удон получать удовольствие, задумчиво под суши so покушать,\n",
      "Суши смаковать, с жадностью на суши so покушать,\n",
      "Маки наслаждаться, задумчиво под удон so отлично\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# English words\n",
    "english_adjectives = ['delicious', 'tasty', 'juicy', 'flavorful', 'aromatic', 'savory']\n",
    "english_nouns = ['sushi', 'roll', 'nigiri', 'maki', 'temaki', 'udon']\n",
    "english_verbs = ['enjoy', 'devour', 'savor', 'relish', 'feast on', 'indulge in']\n",
    "english_adverbs = ['slowly', 'thoughtfully', 'delightfully', 'tenderly', 'eagerly']\n",
    "english_prepositions = ['with', 'on', 'in', 'under', 'beside']\n",
    "english_endings = ['dine', 'fine', 'nine', 'shine', 'mine', 'vine']\n",
    "\n",
    "# Russian words\n",
    "russian_adjectives = ['вкусный', 'ароматный', 'сочный', 'пикантный', 'восхитительный']\n",
    "russian_nouns = ['суши', 'ролл', 'нигири', 'маки', 'темаки', 'удон']\n",
    "russian_verbs = ['наслаждаться', 'полностью разжириться', 'смаковать', 'получать удовольствие', 'поесть до отвала', 'разгуляться']\n",
    "russian_adverbs = ['медленно', 'задумчиво', 'радостно', 'нежно', 'с жадностью']\n",
    "russian_prepositions = ['с', 'на', 'в', 'под', 'рядом с']\n",
    "russian_endings = ['покушать', 'отлично', 'девять', 'светить', 'мое', 'виноградное']\n",
    "\n",
    "def get_language(language):\n",
    "  if language == 'english':\n",
    "    return (english_adjectives, english_nouns, english_verbs, english_adverbs, english_prepositions, english_endings)\n",
    "  elif language == 'russian':\n",
    "    return (russian_adjectives, russian_nouns, russian_verbs, russian_adverbs, russian_prepositions, russian_endings)\n",
    "  else:\n",
    "    raise ValueError(\"Invalid language specified\")\n",
    "\n",
    "def get_word_order(language):\n",
    "  (adjectives, nouns, verbs, adverbs, prepositions, endings) = get_language(language)\n",
    "  word_orders = [\n",
    "    (random.choice(adjectives), random.choice(nouns), random.choice(verbs), random.choice(nouns)),\n",
    "    (random.choice(nouns), random.choice(verbs), random.choice(adverbs), random.choice(prepositions), random.choice(nouns)),\n",
    "    (random.choice(verbs), random.choice(prepositions), random.choice(adjectives), random.choice(nouns)),\n",
    "  ]\n",
    "  return random.choice(word_orders)\n",
    "\n",
    "def generate_line(language, is_last_line):\n",
    "  (adjectives, nouns, verbs, adverbs, prepositions, endings) = get_language(language)\n",
    "  words = get_word_order(language)\n",
    "  ending = random.choice(endings)\n",
    "\n",
    "  if len(words) == 4:\n",
    "    if is_last_line:\n",
    "      line = f\"{words[0].capitalize()} {words[1]}, {words[2]} the {words[3]} so {ending}\"\n",
    "    else:\n",
    "      line = f\"{words[0].capitalize()} {words[1]}, {words[2]} the {words[3]} so {ending},\"\n",
    "  else:\n",
    "    if is_last_line:\n",
    "      line = f\"{words[0].capitalize()} {words[1]}, {words[2]} {words[3]} {words[4]} so {ending}\"\n",
    "    else:\n",
    "      line = f\"{words[0].capitalize()} {words[1]}, {words[2]} {words[3]} {words[4]} so {ending},\"\n",
    "\n",
    "  return line\n",
    "\n",
    "def generate_poem(language, num_lines=4):\n",
    "  poem = \"\"\n",
    "  for i in range(num_lines):\n",
    "    is_last_line = (i == num_lines - 1)\n",
    "    poem += generate_line(language, is_last_line) + \"\\n\"\n",
    "  return poem\n",
    "\n",
    "# Generate a poem in English\n",
    "print(generate_poem('english'))\n",
    "\n",
    "# Generate a poem in Russian\n",
    "print(generate_poem('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\79138\\Mynka\\no-hw\\voice helper\\experiments.ipynb Ячейка 16\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/79138/Mynka/no-hw/voice%20helper/experiments.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHow do I add two numbers in Python?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/79138/Mynka/no-hw/voice%20helper/experiments.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Tokenize the query using the CodeBERT tokenizer\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/79138/Mynka/no-hw/voice%20helper/experiments.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mBertTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmicrosoft/codebert-base\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/79138/Mynka/no-hw/voice%20helper/experiments.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m query_tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(query)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/79138/Mynka/no-hw/voice%20helper/experiments.ipynb#X34sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Convert the query tokens to token IDs and create a PyTorch tensor\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\79138\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1775\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1772\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1775\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained(\n\u001b[0;32m   1776\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   1777\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   1778\u001b[0m     init_configuration,\n\u001b[0;32m   1779\u001b[0m     \u001b[39m*\u001b[39minit_inputs,\n\u001b[0;32m   1780\u001b[0m     use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   1781\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1782\u001b[0m     local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1783\u001b[0m     _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   1784\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1785\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\79138\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1930\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1928\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[0;32m   1929\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1930\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39minit_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minit_kwargs)\n\u001b[0;32m   1931\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   1932\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m   1933\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1934\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1935\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\79138\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:213\u001b[0m, in \u001b[0;36mBertTokenizer.__init__\u001b[1;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     vocab_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    198\u001b[0m ):\n\u001b[0;32m    199\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    200\u001b[0m         do_lower_case\u001b[39m=\u001b[39mdo_lower_case,\n\u001b[0;32m    201\u001b[0m         do_basic_tokenize\u001b[39m=\u001b[39mdo_basic_tokenize,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 213\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49misfile(vocab_file):\n\u001b[0;32m    214\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a vocabulary file at path \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mvocab_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. To load the vocabulary from a Google pretrained\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         )\n\u001b[0;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab \u001b[39m=\u001b[39m load_vocab(vocab_file)\n",
      "File \u001b[1;32mc:\\Users\\79138\\AppData\\Local\\Programs\\Python\\Python310\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[0;32m     31\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load the CodeBERT model\n",
    "model = transformers.BertModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Define the natural language query\n",
    "query = \"How do I add two numbers in Python?\"\n",
    "\n",
    "# Tokenize the query using the CodeBERT tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "query_tokens = tokenizer.tokenize(query)\n",
    "\n",
    "# Convert the query tokens to token IDs and create a PyTorch tensor\n",
    "query_ids = tokenizer.convert_tokens_to_ids(query_tokens)\n",
    "query_tensor = torch.tensor([query_ids])\n",
    "\n",
    "# Define a list of code samples\n",
    "code_samples = [\n",
    "    \"def add(x, y): return x + y\",\n",
    "    \"x + y\",\n",
    "    \"sum([x, y])\",\n",
    "    \"x.add(y)\",\n",
    "    \"x, y = y, x\"\n",
    "]\n",
    "\n",
    "# Tokenize and pad the code samples using the CodeBERT tokenizer\n",
    "encoded_samples = tokenizer.encode(code_samples, padding=True, return_tensors='pt')\n",
    "\n",
    "# Extract the token IDs and attention mask from the encoded sequences\n",
    "code_tensor = encoded_samples['input_ids']\n",
    "attention_mask = encoded_samples['attention_mask']\n",
    "\n",
    "# Use the model to get the embeddings for the query and code samples\n",
    "query_embedding = model(query_tensor, attention_mask=torch.ones_like(query_tensor))[0][:, 0, :]\n",
    "code_embeddings = model(code_tensor, attention_mask=attention_mask)[0][:, 0, :]\n",
    "\n",
    "# Compare the query embedding to the code sample embeddings using cosine similarity\n",
    "similarities = []\n",
    "for code_embedding in code_embeddings:\n",
    "    similarity = 1 - cosine(query_embedding[0], code_embedding)\n",
    "    similarities.append(similarity)\n",
    "\n",
    "# Sort the code samples by similarity and return the top N most similar code samples\n",
    "N = 3\n",
    "top_samples = [code_samples[i] for i in sorted(range(len(similarities)), key=lambda k: similarities[k], reverse=True)[:N]]\n",
    "print(top_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45435424 45435424 45435424 45435424]\n",
      "[45435424 45435424 45435424 45435424]\n",
      "45435424\n",
      "45435424\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "A = np.array([\n",
    "    [16,  3,  2, 13],\n",
    "    [ 5, 10, 11,  8],\n",
    "    [ 9,  6,  7, 12],\n",
    "    [ 4, 15, 14,  1]\n",
    "])\n",
    "\n",
    "\n",
    "A_cubed = np.linalg.matrix_power(A, 5)\n",
    "print(A_cubed.sum(axis=1))\n",
    "print(A_cubed.sum(axis=0))\n",
    "print(sum([A_cubed[i, i] for i in range(A_cubed.shape[0])]))\n",
    "print(sum([A_cubed[i, A_cubed.shape[0] - 1 - i] for i in range(A_cubed.shape[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12dfad86f805d2d41962782fe4cb74c1c6bdc3dd8d611f50435d9035ef53657e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
